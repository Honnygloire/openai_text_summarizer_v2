#  **openai_text_summarizer_v2**  
### *API de rÃ©sumÃ© de texte utilisant un LLM local openâ€‘source via Ollama (phi3, Mistral, etc.)*

---

## ğŸš€ **PrÃ©sentation**

`openai_text_summarizer_v2` est une API FastAPI permettant de **rÃ©sumer automatiquement du texte** en utilisant un **LLM local openâ€‘source** exÃ©cutÃ© via **Ollama**.  
Le projet est conÃ§u pour Ãªtre :

- **modulaire** (pipeline clair et sÃ©parÃ©)  
- **testable** (FakeLLM + tests unitaires)  
- **rapide** (phi3 par dÃ©faut)  
- **professionnel** (architecture propre, logs, API documentÃ©e)  

Il sâ€™agit dâ€™une version amÃ©liorÃ©e et optimisÃ©e du premier prototype, avec une architecture plus robuste et une meilleure gestion des modÃ¨les.

---

## ğŸ§± **Architecture du projet**

```
openai_text_summarizer_v2/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py              # API FastAPI (endpoints)
â”‚   â”œâ”€â”€ summarizer.py       # Pipeline de rÃ©sumÃ©
â”‚   â”œâ”€â”€ llm_client.py       # Client LLM (Ollama + FakeLLM)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_summarizer.py  # Tests du pipeline + FakeLLM
â”‚   â”œâ”€â”€ test_api.py         # Tests de lâ€™API FastAPI
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ examples/               # Exemples dâ€™utilisation
â”œâ”€â”€ requirements.txt        # DÃ©pendances
â””â”€â”€ README.md               # Documentation
```

---

## âš™ï¸ **Fonctionnement du pipeline**

Le pipeline suit 4 Ã©tapes :

1. **RÃ©ception du texte** via lâ€™API  
2. **Nettoyage et prÃ©paration** du texte  
3. **Appel au modÃ¨le local** via Ollama  
4. **Retour du rÃ©sumÃ©** dans un JSON propre

Le pipeline est totalement indÃ©pendant du modÃ¨le utilisÃ©.

---

## ğŸ§  **ModÃ¨les supportÃ©s**

Le projet supporte plusieurs modÃ¨les openâ€‘source via Ollama :

| ModÃ¨le | Avantages | Usage |
|--------|-----------|--------|
| **phi3** | TrÃ¨s rapide, lÃ©ger, idÃ©al pour Mac | ModÃ¨le par dÃ©faut |
| **mistral** | Plus puissant mais trÃ¨s lent | Optionnel |
| **FakeLLM** | InstantanÃ©, idÃ©al pour les tests | UtilisÃ© dans pytest |

---

## ğŸ“¡ **API FastAPI**

Lâ€™API expose un endpoint principal :

### `POST /summarize`

#### Exemple de requÃªte :

```json
{
  "text": "Votre texte long ici..."
}
```

#### Exemple de rÃ©ponse :

```json
{
  "summary": "RÃ©sumÃ© gÃ©nÃ©rÃ© par le modÃ¨le."
}
```

La documentation interactive est disponible sur :

```
http://127.0.0.1:8000/docs
```

---

## ğŸ§ª **Tests unitaires**

Le projet inclut **3 tests essentiels** :

### âœ”ï¸ Test du faux modÃ¨le (FakeLLM)  
VÃ©rifie que FakeLLM renvoie un rÃ©sumÃ© factice.

### âœ”ï¸ Test du pipeline Summarizer  
VÃ©rifie que le pipeline fonctionne mÃªme sans vrai modÃ¨le.

### âœ”ï¸ Test de lâ€™API FastAPI  
VÃ©rifie que `/summarize` renvoie bien un rÃ©sumÃ© (HTTP 200 + champ `summary`).

### Lancer les tests :

```bash
export PYTHONPATH=$(pwd)
pytest -q
```

RÃ©sultat attendu :

```
3 passed
```

---

## ğŸ› ï¸ **Installation**

### 1. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 2. Installer Ollama  
TÃ©lÃ©charger depuis : https://ollama.com

### 3. TÃ©lÃ©charger un modÃ¨le

```bash
ollama pull phi3
```

ou

```bash
ollama pull mistral
```

---

## â–¶ï¸ **Lancer lâ€™API**

```bash
uvicorn app.api:app --reload
```

Lâ€™API dÃ©marre sur :

```
http://127.0.0.1:8000
```

---

## ğŸ¯ **Objectifs atteints**

- API fonctionnelle  
- Pipeline IA robuste  
- ModÃ¨le local rapide  
- Tests unitaires complets  
- Architecture professionnelle  
- Documentation claire  

---

## ğŸš€ **AmÃ©liorations possibles**

- Dockerisation du projet  
- Interface web (Streamlit / React)  
- Choix du modÃ¨le via un paramÃ¨tre API  
- RÃ©sumÃ© court / moyen / long  
- Logging avancÃ©  
